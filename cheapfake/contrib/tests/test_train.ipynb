{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tests the train.py module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time \n",
    "\n",
    "import cv2\n",
    "import torch\n",
    "import torchvision\n",
    "import numpy as np\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "\n",
    "import cheapfake.contrib.dataset as dataset\n",
    "import cheapfake.contrib.models_contrib as models \n",
    "import cheapfake.contrib.transforms as transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing AugmentedFAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded parameters / Total parameters: 24/24\n",
      "torch.Size([1, 4, 18, 17])\n",
      "torch.Size([1, 1224])\n",
      "torch.Size([1, 4, 18, 17])\n",
      "torch.Size([1, 1224])\n",
      "Entire operation took 87.64415907859802 seconds\n",
      "torch.Size([2, 1, 256]) torch.Size([2, 75, 68, 2])\n"
     ]
    }
   ],
   "source": [
    "random_seed = 41\n",
    "metadata_path = \"/home/shu/cheapfake/cheapfake/contrib/wide_balanced_metadata_fs03.csv\"\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "dfdataset = dataset.DeepFakeDataset(metadata_path=metadata_path, frame_transform=transforms.BatchRescale(4), sequential_audio=True, sequential_frames=True, random_seed=random_seed, num_samples=2)\n",
    "dfdataloader = DataLoader(dfdataset, batch_size=2, shuffle=True)\n",
    "\n",
    "for batch_index, batch in enumerate(dfdataloader):\n",
    "    frames, _, audio_stft, label = batch\n",
    "    frames = frames[:, :75]\n",
    "    \n",
    "face_model = models.AugmentedFAN(device=device)\n",
    "frames_model = models.AugmentedLipNet(device=device)\n",
    "\n",
    "frames = frames.float().to(device)\n",
    "start_time = time.time()\n",
    "landmarks, fan_embedding = face_model(frames)\n",
    "end_time = time.time()\n",
    "print(\"Entire operation took {} seconds\".format(end_time - start_time))\n",
    "\n",
    "print(fan_embedding.shape, landmarks.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _find_bounding_box(landmarks, tol=(2, 2, 2, 2)):\n",
    "    \"\"\"Finds the minimum bounding box containing the points, with tolerance in the left, right, top, and bottom directions (in pixels).\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    landmarks : numpy.ndarray or torch.Tensor instance\n",
    "        Numpy array or Torch tensor containing the predicted xy-coordinates of the detected facial landmarks.\n",
    "    tol : tuple, optional\n",
    "        The tolerance (in pixels) in each direction (left, top, right, bottom) by default (2, 2, 2, 2). \n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    bbox : tuple (of ints)\n",
    "        Tuple (min_x, min_y, max_x, max_y) containing the coordinates of the bounding box, with tolerance in the left, right, top and bottom directions. \n",
    "\n",
    "    \"\"\"\n",
    "    assert isinstance(tol, tuple)\n",
    "    assert len(tol) == 4, \"Need four values for the tolerance.\"\n",
    "\n",
    "    x_coords, y_coords = zip(*landmarks)\n",
    "    bbox = (\n",
    "        min(x_coords) - tol[0],\n",
    "        min(y_coords) - tol[1],\n",
    "        max(x_coords) + tol[2],\n",
    "        max(y_coords) + tol[3],\n",
    "    )\n",
    "    bbox = tuple([int(item) for item in bbox])\n",
    "\n",
    "    return bbox\n",
    "\n",
    "\n",
    "def _find_bounding_boxes(landmarks, tol=(2, 2, 2, 2)):\n",
    "    \"\"\"Finds the minimum bounding boxes for a batch of facial landmarks.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    landmarks : numpy.ndarray or torch.Tensor instance\n",
    "        Numpy array or Torch tensor containing the xy-coordinates of the detected facial landmarks, in batches.\n",
    "    tol : tuple, optional\n",
    "        The tolerance (in pixels) in each direction (left, top, right, bottom) by default (2, 2, 2,2).\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    bboxes : list (of tuples)\n",
    "        List containing tuples containing the coordinates of the bounding boxes for the batch of landmarks.\n",
    "\n",
    "    \"\"\"\n",
    "    bboxes = list()\n",
    "    landmarks = landmarks[:, 48:68]\n",
    "    for landmark in landmarks:\n",
    "        bboxes.append(_find_bounding_box(landmark, tol))\n",
    "\n",
    "    return bboxes\n",
    "\n",
    "\n",
    "def _crop_lips(frames, landmarks, tol=(2, 2, 2, 2), channels_first=True):\n",
    "    \"\"\"Crops the lip area from a batch of frames.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    frames : torch.Tensor instance\n",
    "        Torch tensor instance containing the frames to crop the lip areas from.\n",
    "    landmarks : numpy.ndarray or torch.Tensor instance\n",
    "        Numpy array or Torch tensor containing the xy-coordinates of the detected facial landmarks.\n",
    "    tol : tuple, optional\n",
    "        The tolerance (in pixels) in each direction (left, top, right, bottom) by default (2, 2, 2, 2).\n",
    "    channels_first : bool, optional\n",
    "        If True then the input and output are assumed to have shape (sample, channel, height, width), by default True. Otherwise the input and output are assumed to have shape (sample, height, width, channel).\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    cropped_frames : numpy.ndarray or torch.Tensor instance\n",
    "        Numpy array or Torch tensor containing the cropped lips.\n",
    "\n",
    "    \"\"\"\n",
    "    assert isinstance(frames, torch.Tensor)\n",
    "    assert isinstance(landmarks, (torch.Tensor, np.ndarray))\n",
    "    assert isinstance(tol, tuple)\n",
    "    assert isinstance(channels_first, bool)\n",
    "    \n",
    "    if channels_first:\n",
    "        frames = frames.permute(0, 2, 3, 1)\n",
    "\n",
    "    bboxes = _find_bounding_boxes(landmarks, tol=tol)\n",
    "    \n",
    "    extracted_lips = torch.empty(frames.shape[0], 64, 128, 3)\n",
    "    for idx, (bbox, frame) in enumerate(zip(bboxes, frames)):\n",
    "        extracted_lip = frame[bbox[1]:bbox[3], bbox[0]:bbox[2], :]\n",
    "        extracted_lips[idx] = torch.from_numpy(cv2.resize(extracted_lip.cpu().numpy(), dsize=(128, 64), interpolation=cv2.INTER_CUBIC))\n",
    "    \n",
    "    return extracted_lips\n",
    "\n",
    "def _crop_lips_batch(batch_frames, batch_landmarks, tol=(2, 2, 2, 2), channels_first=True):\n",
    "    \"\"\"Extracts the lip area for a batch of batch of frames.\n",
    "    \n",
    "    Finish documentation later.\n",
    "    \n",
    "    \"\"\"\n",
    "    assert isinstance(batch_frames, torch.Tensor)\n",
    "    assert isinstance(batch_landmarks, torch.Tensor)\n",
    "    assert isinstance(tol, tuple)\n",
    "    assert isinstance(channels_first, bool)\n",
    "    \n",
    "    output_shape = (batch_frames.shape[0], batch_frames.shape[1], 64, 128, batch_frames.shape[2])\n",
    "    batch_extracted_lips = torch.empty(output_shape)\n",
    "    for idx, (frames, landmarks) in enumerate(zip(batch_frames, batch_landmarks)):\n",
    "        batch_extracted_lips[idx] = _crop_lips(frames, landmarks, tol=tol, channels_first=channels_first)\n",
    "    \n",
    "    return batch_extracted_lips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "extracted_lips has shape torch.Size([2, 3, 75, 64, 128])\n",
      "[INFO] Starting foward pass\n",
      "[INFO] Finished convolution layer 1\n",
      "[INFO] Finished convolution layer 2\n",
      "[INFO] Finished convolution layer 3\n",
      "[INFO] Finished recurrent unit layers\n",
      "[INFO] Starting fully connected layer\n",
      "[INFO] Finished forward pass\n",
      "Entire process took 0.49370598793029785 seconds\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "extracted_lips = _crop_lips_batch(frames, landmarks)\n",
    "extracted_lips = extracted_lips.permute(0, -1, 1, 2, 3).float().to(device)\n",
    "print(\"extracted_lips has shape {}\".format(extracted_lips.shape))\n",
    "#extracted_lips = extracted_lips.float().to(device)\n",
    "#extracted_lips = extracted_lips.permute(3, 0, 1, 2)\n",
    "#extracted_lips = extracted_lips[None, :, :, :, :]\n",
    "lip_embedding = frames_model(extracted_lips)\n",
    "end_time = time.time()\n",
    "\n",
    "print(\"Entire process took {} seconds\".format(end_time - start_time))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 1, 256])\n"
     ]
    }
   ],
   "source": [
    "print(lip_embedding.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 2, 256])\n"
     ]
    }
   ],
   "source": [
    "concat_features = torch.cat((fan_embedding, lip_embedding), axis=1)\n",
    "print(concat_features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding size is 256, encoder SAP.\n",
      "torch.Size([2, 1, 256])\n"
     ]
    }
   ],
   "source": [
    "import cheapfake.contrib.ResNetSE34L as resnet_models\n",
    "\n",
    "audio_model = resnet_models.ResNetSE34L().to(device)\n",
    "audio_embedding = audio_model(audio_stft.view(audio_stft.shape[0], -1).float().to(device))\n",
    "\n",
    "print(audio_embedding[:, None, :].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "concat_all = torch.cat((fan_embedding, lip_embedding, audio_embedding[:, None, :]), axis=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
