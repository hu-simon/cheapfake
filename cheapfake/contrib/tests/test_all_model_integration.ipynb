{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tests integration of all three models to see if it works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "\n",
    "import torch \n",
    "import numpy as np\n",
    "import face_alignment\n",
    "\n",
    "import cheapfake.contrib.dataset as dataset\n",
    "from cheapfake.contrib.models import CheapFake\n",
    "import cheapfake.contrib.transforms as transforms\n",
    "import cheapfake.contrib.video_processor as video_processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([75, 3, 270, 480])\n",
      "torch.Size([48000])\n",
      "torch.Size([94, 1025])\n"
     ]
    }
   ],
   "source": [
    "random_seed = 41\n",
    "root_path = \"/mnt/fs03/shared/datasets/dfdc_train_all\"\n",
    "\n",
    "dfdataset = dataset.DeepFakeDataset(\n",
    "    root_path=root_path, frame_transform=transforms.BatchRescale(4), sequential_audio=True\n",
    ")\n",
    "frames, audio, audio_stft = dfdataset.__getitem__(0)\n",
    "frames = frames[:75]\n",
    "print(frames.shape)\n",
    "print(audio.shape)\n",
    "print(audio_stft.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CheapFake(device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _forward_fan(x):\n",
    "    fan_output = model.face_alignment_model.get_landmarks_from_batch(x)\n",
    "    predictions = np.asarray(fan_output).squeeze(axis=1)\n",
    "    \n",
    "    # 45 -> 55 is the range of the two lip areas horizontally. \n",
    "    # 52 -> 58 is the range of the two lip areas vertically.\n",
    "    # These are the coordinates of the bounding box that will be used to crop the lip areas.\n",
    "    lip_range_coords = [predictions[:, 45,:], predictions[:, 52,:], predictions[:, 55,:], predictions[:, 58,:]]\n",
    "    \n",
    "    # Crop the images once the coordinates are obtained.\n",
    "    \"\"\"\n",
    "    tolerance = 30\n",
    "    x = np.einsum(\"ijkl->iklj\", x.cpu().numpy())\n",
    "    cropped_lips = torch.empty(x.shape)\n",
    "    for k, frame in enumerate(x):\n",
    "        cropped_lips[k] = torch.from_numpy(frame[\n",
    "            int(lip_range_coords[0][k][0]):int(lip_range_coords[2][k][0]), \n",
    "            int(lip_range_coords[1][k][1]):int(lip_range_coords[3][k][1])\n",
    "        ]).cuda()\n",
    "    \"\"\"\n",
    "    \n",
    "    return predictions, lip_range_coords\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entire operation took 45.01742911338806 seconds\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "predictions, lip_range_coords = _forward_fan(frames.float().cuda())\n",
    "end_time = time.time()\n",
    "\n",
    "print(\"Entire operation took {} seconds\".format(end_time - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "240\n",
      "235\n"
     ]
    }
   ],
   "source": [
    "print(int(lip_range_coords[0][0][0]))\n",
    "print(int(lip_range_coords[2][k][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The expanded size of the tensor (480) must match the existing size (6) at non-singleton dimension 1.  Target sizes: [270, 480, 3].  Tensor sizes: [0, 6, 3]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-6818c5c906cf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mframes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meinsum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"ijkl->iklj\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mframes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mframe\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     cropped_lips[k] = frame[\n\u001b[0m\u001b[1;32m      5\u001b[0m         \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlip_range_coords\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlip_range_coords\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlip_range_coords\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlip_range_coords\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The expanded size of the tensor (480) must match the existing size (6) at non-singleton dimension 1.  Target sizes: [270, 480, 3].  Tensor sizes: [0, 6, 3]"
     ]
    }
   ],
   "source": [
    "cropped_lips = torch.empty((frames.shape[0], frames.shape[2], frames.shape[3], frames.shape[1]))\n",
    "frames = torch.from_numpy(np.einsum(\"ijkl->iklj\", frames))\n",
    "for k, frame in enumerate(frames):\n",
    "    cropped_lips[k] = frame[\n",
    "        int(lip_range_coords[0][k][0]):int(lip_range_coords[2][k][0]),\n",
    "        int(lip_range_coords[1][k][1]):int(lip_range_coords[3][k][1]),\n",
    "    ]\n",
    "\n",
    "print(cropped_lips.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
