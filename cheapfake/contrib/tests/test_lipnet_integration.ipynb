{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python_defaultSpec_1597190815045",
   "display_name": "Python 3.8.3 64-bit"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tests integration of LipNet with the dataset.py classes, and the torch.utils.data.DataLoader class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time \n",
    "import random \n",
    "\n",
    "import cv2\n",
    "import torch\n",
    "import librosa\n",
    "import numpy as np\n",
    "import torchsummary \n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "import cheapfake.lipnet.models as models\n",
    "import cheapfake.contrib.dataset as dataset\n",
    "import cheapfake.contrib.video_processor as video_processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_seed = 41\n",
    "root_path = \"/Users/shu/Documents/Datasets/DFDC_small_subset_raw\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[INFO] Starting foward pass\n[INFO] Finished convolution layer 1\n[INFO] Finished convolution layer 2\n[INFO] Finished convolution layer 3\n[INFO] Finished recurrent unit layers\n[INFO] Starting fully connected layer\n[INFO] Finished forward pass\n----------------------------------------------------------------\n        Layer (type)               Output Shape         Param #\n================================================================\n            Conv3d-1      [1, 32, 75, 135, 240]           7,232\n              ReLU-2      [1, 32, 75, 135, 240]               0\n         Dropout3d-3      [1, 32, 75, 135, 240]               0\n         MaxPool3d-4       [1, 32, 75, 67, 120]               0\n            Conv3d-5       [1, 64, 75, 67, 120]         153,664\n              ReLU-6       [1, 64, 75, 67, 120]               0\n         Dropout3d-7       [1, 64, 75, 67, 120]               0\n         MaxPool3d-8        [1, 64, 75, 33, 60]               0\n            Conv3d-9        [1, 96, 75, 33, 60]         165,984\n             ReLU-10        [1, 96, 75, 33, 60]               0\n        Dropout3d-11        [1, 96, 75, 33, 60]               0\n        MaxPool3d-12        [1, 96, 75, 16, 30]               0\n              GRU-13  [[-1, 2, 512], [-1, 2, 256]]               0\n          Dropout-14                [1, 2, 512]               0\n              GRU-15  [[-1, 2, 512], [-1, 2, 256]]               0\n          Dropout-16                [1, 2, 512]               0\n================================================================\nTotal params: 326,880\nTrainable params: 326,880\nNon-trainable params: 0\n----------------------------------------------------------------\nInput size (MB): 111.24\nForward/backward pass size (MB): 3243.49\nParams size (MB): 1.25\nEstimated Total Size (MB): 3355.97\n----------------------------------------------------------------\nNone\n"
    }
   ],
   "source": [
    "# Test to see what shape is expected by LipNet\n",
    "model = models.LipNet()\n",
    "print(torchsummary.summary(model=model, input_size=(3, 75, 270, 480), batch_size=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like LipNet takes as input a tensor of shape (Color, Frames, Height, Width) or (Color, Frames, Width, Height) though I suspect that it does not matter for the spatial dimensions. Currently, the dataset.py loads the frames in as (Frames, Color, Height, Width) or (Frames, Height, Width, Color) so a pre-processing step needs to be done before feeding into LipNet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing with dataset.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "torch.Size([1, 3, 75, 270, 480])\n[INFO] Starting foward pass\n[INFO] Finished convolution layer 1\n[INFO] Finished convolution layer 2\n[INFO] Finished convolution layer 3\n[INFO] Finished recurrent unit layers\n[INFO] Starting fully connected layer\n[INFO] Finished forward pass\nEntire operation took 28.975037336349487 seconds\ntorch.Size([75, 1, 512])\n"
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "dfdataset = dataset.DeepFakeDataset(\n",
    "    root_path=root_path, \n",
    "    return_tensor=False, \n",
    "    random_seed=random_seed, \n",
    "    sequential_frames=False, \n",
    "    sequential_audio=True, \n",
    "    stochastic=True,\n",
    ")\n",
    "\n",
    "# Need to change the ordering of the frames to match (Color, Frames, Height, Width).abs\n",
    "frames, audio, _ = dfdataset.__getitem__(0)\n",
    "frames = np.einsum(\"ijkl->jikl\", frames[:75])\n",
    "frames = torch.from_numpy(frames)\n",
    "frames = frames[None, :, :, :,]\n",
    "print(frames.shape)\n",
    "prediction = model(frames.float())\n",
    "end_time = time.time()\n",
    "\n",
    "print(\"Entire operation took {} seconds\".format(end_time - start_time))\n",
    "print(prediction.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above works, but some minor modifications had to be made to LipNet's architecture. As a result, this means we either have to change the size of the image to (64, 128) and then use the pretrained weights, or we can rewrite LipNet and then train new network weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}