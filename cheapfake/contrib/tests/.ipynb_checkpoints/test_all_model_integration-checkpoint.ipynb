{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tests integration of all three models to see if it works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "\n",
    "import cv2\n",
    "import torch \n",
    "import numpy as np\n",
    "import face_alignment\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import cheapfake.contrib.dataset as dataset\n",
    "from cheapfake.contrib.models import CheapFake\n",
    "import cheapfake.contrib.transforms as transforms\n",
    "import cheapfake.contrib.video_processor as video_processor\n",
    "\n",
    "%matplotlib notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_seed = 3\n",
    "metadata_path = \"/home/shu/cheapfake/cheapfake/contrib/balanced_metadata_fs03.csv\"\n",
    "\n",
    "dfdataset = dataset.DeepFakeDataset(\n",
    "    metadata_path=metadata_path, frame_transform=transforms.BatchRescale(4), sequential_audio=True,random_seed=random_seed\n",
    ")\n",
    "frames, audio, audio_stft = dfdataset.__getitem__(100)\n",
    "frames = frames[:75]\n",
    "print(frames.shape)\n",
    "print(audio.shape)\n",
    "print(audio_stft.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CheapFake(device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot an image with the FAN features. \n",
    "fan_output = model.face_alignment_model.get_landmarks_from_batch(frames.float().cuda())\n",
    "predictions = np.asarray(fan_output).squeeze(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(predictions.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the predictions to see what indices are needed for the lips.\n",
    "annot = list(range(0, 68))\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.imshow(np.einsum(\"ijk->jki\", frames[0]).astype(\"uint8\"))\n",
    "ax.scatter(predictions[0][:,0], predictions[0][:,1], s=0.5)\n",
    "\n",
    "for i, txt in enumerate(annot):\n",
    "    ax.annotate(txt, (predictions[0][i][0], predictions[0][i][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _find_bounding_box(points, tol=2):\n",
    "    \"\"\"Finds the minimum bounding box of the points. \n",
    "    \n",
    "    \"\"\"\n",
    "    x_coords, y_coords = zip(*points)\n",
    "    min_bbox = (min(x_coords) - tol, min(y_coords) - tol, max(x_coords) + tol, max(y_coords) + tol)\n",
    "    min_bbox = tuple([int(item) for item in min_bbox])\n",
    "    \n",
    "    return min_bbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_bbox = _find_bounding_box(predictions[0][48:68])\n",
    "print(min_bbox)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crop the image according to the minimum bbox.\n",
    "cropped_image = np.einsum(\"ijk->jki\", frames[0])[min_bbox[1]:min_bbox[3], min_bbox[0]:min_bbox[2]]\n",
    "fig, ax = plt.subplots()\n",
    "ax.imshow(cropped_image.astype(\"uint8\"))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape the cropped image to 64 x 128.\n",
    "reshaped_cropped_image = cv2.resize(cropped_image, dsize=(128, 64), interpolation=cv2.INTER_CUBIC)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.imshow(reshaped_cropped_image.astype(np.int16))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _find_bounding_boxes(predictions, tol=2):\n",
    "    bboxes = list()\n",
    "    predictions = predictions[:,48:68]\n",
    "    for prediction in predictions:\n",
    "        bboxes.append(_find_bounding_box(points=prediction, tol=tol))\n",
    "        \n",
    "    return bboxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _crop_lips(frames, landmarks, tol=2):\n",
    "    frames = np.einsum(\"ijkl->iklj\", frames.cpu().numpy())\n",
    "    bboxes = _find_bounding_boxes(predictions=landmarks, tol=tol)\n",
    "    cropped_frames = torch.empty((frames.shape[0], 64, 128, frames.shape[-1]))\n",
    "    for k, (bbox, frame) in enumerate(zip(bboxes, frames)):\n",
    "        cropped_frame = frame[bbox[1]:bbox[3], bbox[0]:bbox[2], :]\n",
    "        cropped_frames[k] = torch.from_numpy(cv2.resize(cropped_frame, dsize=(128, 64), interpolation=cv2.INTER_CUBIC))\n",
    "        \n",
    "    return cropped_frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _forward_fan(x):\n",
    "    fan_output = model.face_alignment_model.get_landmarks_from_batch(x)\n",
    "    fan_output = np.asarray(fan_output).squeeze(axis=1)\n",
    "    \n",
    "    cropped_frames = _crop_lips(x, fan_output)\n",
    "    \n",
    "    return predictions, cropped_frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "predictions, cropped_frames = _forward_fan(frames.float().cuda())\n",
    "end_time = time.time()\n",
    "\n",
    "print(\"Entire operation took {} seconds\".format(end_time - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 3)\n",
    "ax[0].imshow(cropped_frames[0].cpu().numpy().astype(np.int16))\n",
    "ax[1].imshow(cropped_frames[1].cpu().numpy().astype(np.int16))\n",
    "ax[2].imshow(cropped_frames[2].cpu().numpy().astype(np.int16))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entire operation took 4.488544940948486 seconds\n"
     ]
    }
   ],
   "source": [
    "# Confirm that everything works now that the above work is in models.py\n",
    "random_seed=41\n",
    "metadata_path = \"/home/shu/cheapfake/cheapfake/contrib/balanced_metadata_fs03.csv\"\n",
    "dfdataset = dataset.DeepFakeDataset(metadata_path=metadata_path, sequential_audio=True, random_seed=random_seed, frame_transform=transforms.BatchRescale(4))\n",
    "\n",
    "start_time = time.time()\n",
    "frames, audio, audio_stft = dfdataset.__getitem__(0)\n",
    "end_time = time.time()\n",
    "print(\"Entire operation took {} seconds\".format(end_time - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "75\n",
      "Entire operation took 42.38333749771118 seconds\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = CheapFake(device=device)\n",
    "\n",
    "start_time = time.time()\n",
    "fan_output, cropped_lips = model(frames[:75].float().cuda())\n",
    "end_time = time.time()\n",
    "\n",
    "print(\"Entire operation took {} seconds\".format(end_time - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 3)\n",
    "ax[0].imshow(cropped_lips[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
