{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Python script that trains the network.\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import time\n",
    "from pathlib import Path\n",
    "\n",
    "import cv2\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "\n",
    "import cheapfake.contrib.dataset as dataset\n",
    "import cheapfake.contrib.models_contrib as models\n",
    "import cheapfake.contrib.transforms as transforms\n",
    "import cheapfake.contrib.ResNetSE34L as resnet_models\n",
    "\n",
    "__all__ = [\"train_model\", \"eval_model\"]\n",
    "\n",
    "\n",
    "def save_checkpoints(face_model, frame_model, audio_model, description, filename):\n",
    "    \"\"\"Saves the current state of the network weights to a checkpoint file.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    face_model : torch.nn.Module instance\n",
    "        A torch.nn.Module instance containing the model weights of the face alignment/embedding network.\n",
    "    frame_model : torch.nn.Module instance\n",
    "        A torch.nn.Module instance containing the model weights of the frames/lips embedding network\n",
    "    audio_model : torch.nn.Module instance\n",
    "        A torch.nn.Module instance containing the model weights of the audio embedding network.\n",
    "    description : str\n",
    "        String describing the saved checkpoint.\n",
    "    filename : str\n",
    "        The name of the file to be saved. The suffix should be included in the filename.\n",
    "\n",
    "    \"\"\"\n",
    "    assert isinstance(face_model, torch.nn.Module)\n",
    "    assert isinstance(frame_model, torch.nn.Module)\n",
    "    assert isinstance(audio_model, torch.nn.Module)\n",
    "    assert isinstance(description, str)\n",
    "    assert isinstance(filename, str)\n",
    "\n",
    "    model_state = {\n",
    "        \"description\": description,\n",
    "        \"face_model\": face_model.state_dict(),\n",
    "        \"frame_model\": frame_model.state_dict(),\n",
    "        \"audio_model\": audio_model.state_dict(),\n",
    "    }\n",
    "\n",
    "    torch.save(state, filename)\n",
    "\n",
    "\n",
    "'''\n",
    "def train_model(\n",
    "    face_model,\n",
    "    frames_model,\n",
    "    audio_model,\n",
    "    dataloader,\n",
    "    optimizer,\n",
    "    criterion,\n",
    "    num_epochs,\n",
    "    device=torch.device(\"cpu\"),\n",
    "    verbose=True,\n",
    "):\n",
    "    \"\"\"Trains the DeepFake detection model.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    face_model : torch.nn.Module instance\n",
    "        A torch.nn.Module instance used to create the face embeddings.\n",
    "    frames_model : torch.nn.Module instance\n",
    "        A torch.nn.Module instance used to create the frames/lips embeddings.\n",
    "    audio_model : torch.nn.Module instance\n",
    "        A torch.nn.Module instance used to create the audio embeddings.\n",
    "    dataloader : torch.utils.data.dataloader.DataLoader instance\n",
    "        Torch dataloader used for loading the training data.\n",
    "    optimizer : torch.optim instance\n",
    "        Torch optimizer function used for gradient descent.\n",
    "    criterion : torch.nn Loss Function instance\n",
    "        A torch.nn loss function used for gradient descent.\n",
    "    num_epochs : int\n",
    "        The number of epochs for training.\n",
    "    device : torch.device instance\n",
    "        The device on which all the computations are done.\n",
    "    verbose : {True, False}, bool, optional\n",
    "        If True, then training statistics are printed to the system console.\n",
    "    \n",
    "    \"\"\"\n",
    "    assert isinstance(frame_model, torch.nn.Module)\n",
    "    assert isinstance(lip_model, torch.nn.Module)\n",
    "    assert isinstance(audio_model, torch.nn.Module)\n",
    "    assert isinstance(device, torch.device)\n",
    "    assert isinstance(verbose, bool)\n",
    "\n",
    "    frame_model = frame_model.to(device)\n",
    "    lip_model = lip_model.to(device)\n",
    "    audio_model = audio_model.to(device)\n",
    "\n",
    "    checkpoint_path = \"./checkpoints\"\n",
    "    Path(checkpoint_path).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    losses = list()\n",
    "\n",
    "    if verbose is False:\n",
    "        progress_bar = tqdm(total=len(dataloader))\n",
    "    for epoch in range(n_epochs):\n",
    "        face_model.train()\n",
    "        frames_model.train()\n",
    "        audio_model.train()\n",
    "\n",
    "        for batch_idx, batch in enumerate(dataloader):\n",
    "            frames, _, audio_stft = batch\n",
    "\n",
    "            frames = frames.float().to(device)\n",
    "            audio_stft = audio_stft.float().to(device)\n",
    "\n",
    "            face_model.train()\n",
    "            frames_model.train()\n",
    "            audio_model.train()\n",
    "\n",
    "            optim.zero_grad()\n",
    "\n",
    "            landmarks, face_embedding = face_model(frames)\n",
    "            extracted_lips = _crop_lips(frames, landmarks)\n",
    "            frame_embedding = frames_model(extracted_lips)\n",
    "            audio_embedding = audio_model(audio_stft.view(audio_stft.shape[0], -1))\n",
    "\n",
    "            print(\n",
    "                \"\\nFace Embedding Size: {}\\nFrames Embedding Size: {}\\nAudio Embedding Size: {}\".format(\n",
    "                    face_embedding.shape, frame_embedding.shape, audio_embedding.shape\n",
    "                )\n",
    "            )\n",
    "\n",
    "            # Compute the loss and then take the gradient step. Compute some verbose output if you want, especially if the user requests it.\n",
    "'''\n",
    "\n",
    "\n",
    "def train_model(\n",
    "    face_model,\n",
    "    frame_model,\n",
    "    audio_model,\n",
    "    dataloader,\n",
    "    optimizer,\n",
    "    criterion,\n",
    "    num_epochs,\n",
    "    checkpoint_path,\n",
    "    device=torch.device(\"cpu\"),\n",
    "    verbose=True,\n",
    "):\n",
    "    \"\"\"Trains the DeepFake detection model.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    face_model : torch.nn.Module instance\n",
    "        Torch module used to create the face embeddings.\n",
    "    frame_model : torch.nn.Module instance\n",
    "        Torch module used to create the lip embeddings.\n",
    "    audio_model : torch.nn.Module instance\n",
    "        Torch module used to create the audio embedddings.\n",
    "    dataloader : torch.utils.data.dataloader.DataLoader instance\n",
    "        Torch dataloader used to load the training data.\n",
    "    optimizer : torch.optim instance\n",
    "        Torch optimizer used for gradient descent.\n",
    "    num_epochs : int\n",
    "        The number of epochs for training.\n",
    "    checkpoint_path : str\n",
    "        The absolute path to the folder where checkpoints should be stored.\n",
    "    device : torch.device instance, optional\n",
    "        The device on which all procedures are carried out.\n",
    "    verbose : bool, optional\n",
    "        If True then training statistics are printed to the system console.\n",
    "\n",
    "    \"\"\"\n",
    "    assert isinstance(face_model, torch.nn.Module)\n",
    "    assert isinstance(frame_model, torch.nn.Module)\n",
    "    assert isinstance(audio_model, torch.nn.Module)\n",
    "    assert isinstance(num_epochs, int)\n",
    "    assert isinstance(checkpoint_path, str)\n",
    "    assert isinstance(device, torch.device)\n",
    "    assert isinstance(verbose, bool)\n",
    "\n",
    "    #face_model = face_model.to(device)\n",
    "    #frame_model = frame_model.to(device)\n",
    "    #audio_model = audio_model.to(device)\n",
    "\n",
    "    Path(checkpoint_path).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    losses = list()\n",
    "\n",
    "    if verbose is False:\n",
    "        progress_bar = tqdm(total=len(dataloader))\n",
    "    for epoch in range(num_epochs):\n",
    "        face_model.train()\n",
    "        frame_model.train()\n",
    "        audio_model.train()\n",
    "\n",
    "        for batch_idx, batch in enumerate(dataloader):\n",
    "            face_model.train()\n",
    "            frame_model.train()\n",
    "            audio_model.train()\n",
    "\n",
    "            frames, _, audio_stft, label = batch\n",
    "            frames = frames[:, :75]\n",
    "            frames = frames.float().to(device)\n",
    "            audio_stft = audio_stft.view(audio_stft.shape[0], -1).float().to(device)\n",
    "\n",
    "            #optim.zero_grad()\n",
    "            \n",
    "            print(\"Going through FAN\")\n",
    "            landmarks, face_embeddings = face_model(frames)\n",
    "            \n",
    "            print(\"Going through LipNet\")\n",
    "            extracted_lips = frame_model._crop_lips_batch(frames, landmarks)\n",
    "            extracted_lips = extracted_lips.permute(0, -1, 1, 2, 3)\n",
    "            extracted_lips = extracted_lips.to(device)\n",
    "            print(extracted_lips.shape)\n",
    "            frame_embeddings = frame_model(extracted_lips)\n",
    "            \n",
    "            print(\"Going through ResNet\")\n",
    "            audio_embeddings = audio_model(audio_stft)\n",
    "\n",
    "            # Concatenate the embeddings together.\n",
    "            concat_embeddings = (\n",
    "                torch.cat(\n",
    "                    (face_embeddings, frame_embeddings, audio_embeddings[:, None, :]),\n",
    "                    axis=1,\n",
    "                )\n",
    "                .float()\n",
    "                .to(device)\n",
    "            )\n",
    "\n",
    "            print(concat_embeddings.shape)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    random_seed = 41\n",
    "    metadata_path = (\n",
    "        \"/home/shu/cheapfake/cheapfake/contrib/wide_balanced_metadata_fs03.csv\"\n",
    "    )\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(\"Using device: {}\".format(device))\n",
    "\n",
    "    dfdataset = dataset.DeepFakeDataset(\n",
    "        metadata_path=metadata_path,\n",
    "        frame_transform=transforms.BatchRescale(4),\n",
    "        sequential_audio=True,\n",
    "        sequential_frames=True,\n",
    "        random_seed=random_seed,\n",
    "        num_samples=1,\n",
    "    )\n",
    "    dfdataloader = DataLoader(dfdataset, batch_size=1, shuffle=True)\n",
    "    checkpoint_path = \"./checkpoints\"\n",
    "\n",
    "    optimizer = 0\n",
    "    criterion = 0\n",
    "    num_epochs = 5\n",
    "\n",
    "    face_model = models.AugmentedFAN(device=device)\n",
    "    frame_model = models.AugmentedLipNet(device=device)\n",
    "    audio_model = resnet_models.ResNetSE34L()\n",
    "\n",
    "    train_model(\n",
    "        face_model=face_model,\n",
    "        frame_model=frame_model,\n",
    "        audio_model=audio_model,\n",
    "        dataloader=dfdataloader,\n",
    "        optimizer=optimizer,\n",
    "        criterion=criterion,\n",
    "        num_epochs=num_epochs,\n",
    "        checkpoint_path=checkpoint_path,\n",
    "    )\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
