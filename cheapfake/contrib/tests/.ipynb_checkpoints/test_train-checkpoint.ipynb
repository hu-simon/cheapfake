{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tests the train.py module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time \n",
    "\n",
    "import cv2\n",
    "import torch\n",
    "import torchvision\n",
    "import numpy as np\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "\n",
    "import cheapfake.contrib.dataset as dataset\n",
    "import cheapfake.contrib.models_contrib as models \n",
    "import cheapfake.contrib.transforms as transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing AugmentedFAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 10, 3, 270, 480])\n",
      "Loaded parameters / Total parameters: 24/24\n",
      "torch.Size([10, 1, 68, 2])\n",
      "torch.Size([10, 1, 68, 2])\n",
      "Only keeping one face.\n",
      "torch.Size([10, 1, 68, 2])\n",
      "Entire operation took 7.1331493854522705 seconds\n",
      "torch.Size([3, 10, 68, 2])\n"
     ]
    }
   ],
   "source": [
    "random_seed = 41\n",
    "metadata_path = \"/home/shu/cheapfake/cheapfake/contrib/wide_balanced_metadata_fs03.csv\"\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "dfdataset = dataset.DeepFakeDataset(metadata_path=metadata_path, frame_transform=transforms.BatchRescale(4), sequential_audio=True, sequential_frames=True, random_seed=random_seed, num_samples=9)\n",
    "dfdataloader = DataLoader(dfdataset, batch_size=3, shuffle=True)\n",
    "\n",
    "for batch_index, batch in enumerate(dfdataloader):\n",
    "    frames, _, audio_stft, label = batch\n",
    "    frames = frames[:, :10]\n",
    "    \n",
    "print(frames.shape)\n",
    "face_model = models.AugmentedFAN(device=device)\n",
    "frames_model = models.AugmentedLipNet(device=device)\n",
    "\n",
    "#frames = frames.reshape([frames.shape[0] * frames.shape[1], frames.shape[2], frames.shape[3], frames.shape[4]]).float().to(device)\n",
    "#print(\"Updated frame shape is: {}\".format(frames.shape))\n",
    "\n",
    "frames = frames.float().to(device)\n",
    "start_time = time.time()\n",
    "landmarks, fan_embedding = face_model(frames)\n",
    "end_time = time.time()\n",
    "print(\"Entire operation took {} seconds\".format(end_time - start_time))\n",
    "\n",
    "print(fan_embedding.shape, landmarks.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 3, 270, 480])\n",
      "torch.Size([10, 3, 270, 480])\n",
      "torch.Size([10, 3, 270, 480])\n"
     ]
    }
   ],
   "source": [
    "for batch in frames:\n",
    "    print(batch.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def _find_bounding_box(landmarks, tol=(2, 2, 2, 2)):\n",
    "    \"\"\"Finds the minimum bounding box containing the points, with tolerance in the left, right, top, and bottom directions (in pixels).\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    landmarks : numpy.ndarray or torch.Tensor instance\n",
    "        Numpy array or Torch tensor containing the predicted xy-coordinates of the detected facial landmarks.\n",
    "    tol : tuple, optional\n",
    "        The tolerance (in pixels) in each direction (left, top, right, bottom) by default (2, 2, 2, 2). \n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    bbox : tuple (of ints)\n",
    "        Tuple (min_x, min_y, max_x, max_y) containing the coordinates of the bounding box, with tolerance in the left, right, top and bottom directions. \n",
    "\n",
    "    \"\"\"\n",
    "    assert isinstance(tol, tuple)\n",
    "    assert len(tol) == 4, \"Need four values for the tolerance.\"\n",
    "\n",
    "    x_coords, y_coords = zip(*landmarks)\n",
    "    bbox = (\n",
    "        min(x_coords) - tol[0],\n",
    "        min(y_coords) - tol[1],\n",
    "        max(x_coords) + tol[2],\n",
    "        max(y_coords) + tol[3],\n",
    "    )\n",
    "    bbox = tuple([int(item) for item in bbox])\n",
    "\n",
    "    return bbox\n",
    "\n",
    "\n",
    "def _find_bounding_boxes(landmarks, tol=(2, 2, 2, 2)):\n",
    "    \"\"\"Finds the minimum bounding boxes for a batch of facial landmarks.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    landmarks : numpy.ndarray or torch.Tensor instance\n",
    "        Numpy array or Torch tensor containing the xy-coordinates of the detected facial landmarks, in batches.\n",
    "    tol : tuple, optional\n",
    "        The tolerance (in pixels) in each direction (left, top, right, bottom) by default (2, 2, 2,2).\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    bboxes : list (of tuples)\n",
    "        List containing tuples containing the coordinates of the bounding boxes for the batch of landmarks.\n",
    "\n",
    "    \"\"\"\n",
    "    bboxes = list()\n",
    "    landmarks = landmarks[:, 48:68]\n",
    "    for landmark in landmarks:\n",
    "        bboxes.append(_find_bounding_box(landmark, tol))\n",
    "\n",
    "    return bboxes\n",
    "\n",
    "\n",
    "def _crop_lips(frames, landmarks, tol=(2, 2, 2, 2), channels_first=True):\n",
    "    \"\"\"Crops the lip area from a batch of frames.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    frames : torch.Tensor instance\n",
    "        Torch tensor instance containing the frames to crop the lip areas from.\n",
    "    landmarks : numpy.ndarray or torch.Tensor instance\n",
    "        Numpy array or Torch tensor containing the xy-coordinates of the detected facial landmarks.\n",
    "    tol : tuple, optional\n",
    "        The tolerance (in pixels) in each direction (left, top, right, bottom) by default (2, 2, 2, 2).\n",
    "    channels_first : bool, optional\n",
    "        If True then the input and output are assumed to have shape (sample, channel, height, width), by default True. Otherwise the input and output are assumed to have shape (sample, height, width, channel).\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    cropped_frames : numpy.ndarray or torch.Tensor instance\n",
    "        Numpy array or Torch tensor containing the cropped lips.\n",
    "\n",
    "    \"\"\"\n",
    "    assert isinstance(frames, torch.Tensor)\n",
    "    assert isinstance(landmarks, (torch.Tensor, np.ndarray))\n",
    "    assert isinstance(tol, tuple)\n",
    "    assert isinstance(channels_first, bool)\n",
    "    \n",
    "    if channels_first:\n",
    "        frames = frames.permute(0, 2, 3, 1)\n",
    "\n",
    "    bboxes = _find_bounding_boxes(landmarks, tol=tol)\n",
    "    \n",
    "    extracted_lips = torch.empty(frames.shape[0], 64, 128, 3)\n",
    "    for idx, (bbox, frame) in enumerate(zip(bboxes, frames)):\n",
    "        extracted_lip = frame[bbox[1]:bbox[3], bbox[0]:bbox[2], :]\n",
    "        extracted_lips[idx] = torch.from_numpy(cv2.resize(extracted_lip.cpu().numpy(), dsize=(128, 64), interpolation=cv2.INTER_CUBIC))\n",
    "    \n",
    "    return extracted_lips\n",
    "\n",
    "def _crop_lips_batch(batch_frames, batch_landmarks, tol=(2, 2, 2, 2), channels_first=True):\n",
    "    \"\"\"Extracts the lip area for a batch of batch of frames.\n",
    "    \n",
    "    Finish documentation later.\n",
    "    \n",
    "    \"\"\"\n",
    "    assert isinstance(batch_frames, torch.Tensor)\n",
    "    assert isinstance(batch_landmarks, torch.Tensor)\n",
    "    assert isinstance(tol, tuple)\n",
    "    assert isinstance(channels_first, bool)\n",
    "    \n",
    "    batch_extracted_lips = torch.empty(batch_frames.shape).permute(0, 1, 2, 3, 4)\n",
    "    for idx, (frames, landmarks) in enumerate(zip(batch_frames, batch_landmarks)):\n",
    "        batch_extracted_lips[idx] = _crop_lips(frames, landmarks, tol=tol, channels_first=channels_first)\n",
    "    \n",
    "    return batch_extracted_lips"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "print(frames_reshaped.shape)\n",
    "print(landmarks.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "start_time = time.time()\n",
    "extracted_lips = _crop_lips(frames_reshaped, landmarks)\n",
    "extracted_lips = extracted_lips.float().to(device)\n",
    "extracted_lips = extracted_lips.permute(3, 0, 1, 2)\n",
    "extracted_lips = extracted_lips[None, :, :, :, :]\n",
    "lip_embedding = frames_model(extracted_lips)\n",
    "end_time = time.time()\n",
    "\n",
    "print(\"Entire process took {} seconds\".format(end_time - start_time))\n",
    "print(lip_embedding.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
