{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda:0\n",
      "Loaded parameters / Total parameters: 24/24\n",
      "Embedding size is 256, encoder SAP.\n",
      "Going through FAN\n",
      "Going through LipNet\n",
      "torch.Size([75, 68, 2])\n",
      "torch.Size([1, 3, 75, 64, 128])\n",
      "[INFO] Starting foward pass\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Could not run 'aten::slow_conv3d_forward' with arguments from the 'CUDA' backend. 'aten::slow_conv3d_forward' is only available for these backends: [CPU, Autograd, Profiler, Tracer].",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-bd1dc3cf452c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    262\u001b[0m     \u001b[0maudio_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresnet_models\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mResNetSE34L\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    263\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 264\u001b[0;31m     train_model(\n\u001b[0m\u001b[1;32m    265\u001b[0m         \u001b[0mface_model\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mface_model\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    266\u001b[0m         \u001b[0mframe_model\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mframe_model\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-1-bd1dc3cf452c>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(face_model, frame_model, audio_model, dataloader, optimizer, criterion, num_epochs, checkpoint_path, device, verbose)\u001b[0m\n\u001b[1;32m    217\u001b[0m             \u001b[0mextracted_lips\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mextracted_lips\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mextracted_lips\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 219\u001b[0;31m             \u001b[0mframe_embeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mframe_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mextracted_lips\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    220\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    221\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Going through ResNet\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/torch/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    720\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 722\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    723\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    724\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/cheapfake/cheapfake/contrib/models_contrib.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    316\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    317\u001b[0m         \"\"\"\n\u001b[0;32m--> 318\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlipnet_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    319\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    320\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/torch/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    720\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 722\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    723\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    724\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/cheapfake/cheapfake/lipnet/models.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    173\u001b[0m             \u001b[0mThe\u001b[0m \u001b[0moutput\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mnetwork\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mforward\u001b[0m \u001b[0;32mpass\u001b[0m \u001b[0mof\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mx\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0mthrough\u001b[0m \u001b[0mLipNet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m         \"\"\"\n\u001b[0;32m--> 175\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lipnet_pre_fc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    176\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"[INFO] Starting fully connected layer\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/cheapfake/cheapfake/lipnet/models.py\u001b[0m in \u001b[0;36m_lipnet_pre_fc\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    124\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"[INFO] Starting foward pass\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 126\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    127\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout3d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/torch/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    720\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 722\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    723\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    724\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/torch/lib/python3.8/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    564\u001b[0m                             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_triple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    565\u001b[0m                             self.dilation, self.groups)\n\u001b[0;32m--> 566\u001b[0;31m         return F.conv3d(input, self.weight, self.bias, self.stride,\n\u001b[0m\u001b[1;32m    567\u001b[0m                         self.padding, self.dilation, self.groups)\n\u001b[1;32m    568\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Could not run 'aten::slow_conv3d_forward' with arguments from the 'CUDA' backend. 'aten::slow_conv3d_forward' is only available for these backends: [CPU, Autograd, Profiler, Tracer]."
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Python script that trains the network.\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import time\n",
    "from pathlib import Path\n",
    "\n",
    "import cv2\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "\n",
    "import cheapfake.contrib.dataset as dataset\n",
    "import cheapfake.contrib.models_contrib as models\n",
    "import cheapfake.contrib.transforms as transforms\n",
    "import cheapfake.contrib.ResNetSE34L as resnet_models\n",
    "\n",
    "__all__ = [\"train_model\", \"eval_model\"]\n",
    "\n",
    "\n",
    "def save_checkpoints(face_model, frame_model, audio_model, description, filename):\n",
    "    \"\"\"Saves the current state of the network weights to a checkpoint file.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    face_model : torch.nn.Module instance\n",
    "        A torch.nn.Module instance containing the model weights of the face alignment/embedding network.\n",
    "    frame_model : torch.nn.Module instance\n",
    "        A torch.nn.Module instance containing the model weights of the frames/lips embedding network\n",
    "    audio_model : torch.nn.Module instance\n",
    "        A torch.nn.Module instance containing the model weights of the audio embedding network.\n",
    "    description : str\n",
    "        String describing the saved checkpoint.\n",
    "    filename : str\n",
    "        The name of the file to be saved. The suffix should be included in the filename.\n",
    "\n",
    "    \"\"\"\n",
    "    assert isinstance(face_model, torch.nn.Module)\n",
    "    assert isinstance(frame_model, torch.nn.Module)\n",
    "    assert isinstance(audio_model, torch.nn.Module)\n",
    "    assert isinstance(description, str)\n",
    "    assert isinstance(filename, str)\n",
    "\n",
    "    model_state = {\n",
    "        \"description\": description,\n",
    "        \"face_model\": face_model.state_dict(),\n",
    "        \"frame_model\": frame_model.state_dict(),\n",
    "        \"audio_model\": audio_model.state_dict(),\n",
    "    }\n",
    "\n",
    "    torch.save(state, filename)\n",
    "\n",
    "\n",
    "'''\n",
    "def train_model(\n",
    "    face_model,\n",
    "    frames_model,\n",
    "    audio_model,\n",
    "    dataloader,\n",
    "    optimizer,\n",
    "    criterion,\n",
    "    num_epochs,\n",
    "    device=torch.device(\"cpu\"),\n",
    "    verbose=True,\n",
    "):\n",
    "    \"\"\"Trains the DeepFake detection model.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    face_model : torch.nn.Module instance\n",
    "        A torch.nn.Module instance used to create the face embeddings.\n",
    "    frames_model : torch.nn.Module instance\n",
    "        A torch.nn.Module instance used to create the frames/lips embeddings.\n",
    "    audio_model : torch.nn.Module instance\n",
    "        A torch.nn.Module instance used to create the audio embeddings.\n",
    "    dataloader : torch.utils.data.dataloader.DataLoader instance\n",
    "        Torch dataloader used for loading the training data.\n",
    "    optimizer : torch.optim instance\n",
    "        Torch optimizer function used for gradient descent.\n",
    "    criterion : torch.nn Loss Function instance\n",
    "        A torch.nn loss function used for gradient descent.\n",
    "    num_epochs : int\n",
    "        The number of epochs for training.\n",
    "    device : torch.device instance\n",
    "        The device on which all the computations are done.\n",
    "    verbose : {True, False}, bool, optional\n",
    "        If True, then training statistics are printed to the system console.\n",
    "    \n",
    "    \"\"\"\n",
    "    assert isinstance(frame_model, torch.nn.Module)\n",
    "    assert isinstance(lip_model, torch.nn.Module)\n",
    "    assert isinstance(audio_model, torch.nn.Module)\n",
    "    assert isinstance(device, torch.device)\n",
    "    assert isinstance(verbose, bool)\n",
    "\n",
    "    frame_model = frame_model.to(device)\n",
    "    lip_model = lip_model.to(device)\n",
    "    audio_model = audio_model.to(device)\n",
    "\n",
    "    checkpoint_path = \"./checkpoints\"\n",
    "    Path(checkpoint_path).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    losses = list()\n",
    "\n",
    "    if verbose is False:\n",
    "        progress_bar = tqdm(total=len(dataloader))\n",
    "    for epoch in range(n_epochs):\n",
    "        face_model.train()\n",
    "        frames_model.train()\n",
    "        audio_model.train()\n",
    "\n",
    "        for batch_idx, batch in enumerate(dataloader):\n",
    "            frames, _, audio_stft = batch\n",
    "\n",
    "            frames = frames.float().to(device)\n",
    "            audio_stft = audio_stft.float().to(device)\n",
    "\n",
    "            face_model.train()\n",
    "            frames_model.train()\n",
    "            audio_model.train()\n",
    "\n",
    "            optim.zero_grad()\n",
    "\n",
    "            landmarks, face_embedding = face_model(frames)\n",
    "            extracted_lips = _crop_lips(frames, landmarks)\n",
    "            frame_embedding = frames_model(extracted_lips)\n",
    "            audio_embedding = audio_model(audio_stft.view(audio_stft.shape[0], -1))\n",
    "\n",
    "            print(\n",
    "                \"\\nFace Embedding Size: {}\\nFrames Embedding Size: {}\\nAudio Embedding Size: {}\".format(\n",
    "                    face_embedding.shape, frame_embedding.shape, audio_embedding.shape\n",
    "                )\n",
    "            )\n",
    "\n",
    "            # Compute the loss and then take the gradient step. Compute some verbose output if you want, especially if the user requests it.\n",
    "'''\n",
    "\n",
    "\n",
    "def train_model(\n",
    "    face_model,\n",
    "    frame_model,\n",
    "    audio_model,\n",
    "    dataloader,\n",
    "    optimizer,\n",
    "    criterion,\n",
    "    num_epochs,\n",
    "    checkpoint_path,\n",
    "    device=torch.device(\"cpu\"),\n",
    "    verbose=True,\n",
    "):\n",
    "    \"\"\"Trains the DeepFake detection model.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    face_model : torch.nn.Module instance\n",
    "        Torch module used to create the face embeddings.\n",
    "    frame_model : torch.nn.Module instance\n",
    "        Torch module used to create the lip embeddings.\n",
    "    audio_model : torch.nn.Module instance\n",
    "        Torch module used to create the audio embedddings.\n",
    "    dataloader : torch.utils.data.dataloader.DataLoader instance\n",
    "        Torch dataloader used to load the training data.\n",
    "    optimizer : torch.optim instance\n",
    "        Torch optimizer used for gradient descent.\n",
    "    num_epochs : int\n",
    "        The number of epochs for training.\n",
    "    checkpoint_path : str\n",
    "        The absolute path to the folder where checkpoints should be stored.\n",
    "    device : torch.device instance, optional\n",
    "        The device on which all procedures are carried out.\n",
    "    verbose : bool, optional\n",
    "        If True then training statistics are printed to the system console.\n",
    "\n",
    "    \"\"\"\n",
    "    assert isinstance(face_model, torch.nn.Module)\n",
    "    assert isinstance(frame_model, torch.nn.Module)\n",
    "    assert isinstance(audio_model, torch.nn.Module)\n",
    "    assert isinstance(num_epochs, int)\n",
    "    assert isinstance(checkpoint_path, str)\n",
    "    assert isinstance(device, torch.device)\n",
    "    assert isinstance(verbose, bool)\n",
    "\n",
    "    #face_model = face_model.to(device)\n",
    "    #frame_model = frame_model.to(device)\n",
    "    #audio_model = audio_model.to(device)\n",
    "\n",
    "    Path(checkpoint_path).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    losses = list()\n",
    "\n",
    "    if verbose is False:\n",
    "        progress_bar = tqdm(total=len(dataloader))\n",
    "    for epoch in range(num_epochs):\n",
    "        face_model.train()\n",
    "        frame_model.train()\n",
    "        audio_model.train()\n",
    "\n",
    "        for batch_idx, batch in enumerate(dataloader):\n",
    "            face_model.train()\n",
    "            frame_model.train()\n",
    "            audio_model.train()\n",
    "\n",
    "            frames, _, audio_stft, label = batch\n",
    "            frames = frames[:, :75]\n",
    "            frames = frames.float().to(device)\n",
    "            audio_stft = audio_stft.view(audio_stft.shape[0], -1).float().to(device)\n",
    "\n",
    "            #optim.zero_grad()\n",
    "            \n",
    "            print(\"Going through FAN\")\n",
    "            landmarks, face_embeddings = face_model(frames)\n",
    "            \n",
    "            print(\"Going through LipNet\")\n",
    "            extracted_lips = frame_model._crop_lips_batch(frames, landmarks)\n",
    "            extracted_lips = extracted_lips.permute(0, -1, 1, 2, 3)\n",
    "            extracted_lips = extracted_lips.to(device)\n",
    "            print(extracted_lips.shape)\n",
    "            frame_embeddings = frame_model(extracted_lips)\n",
    "            \n",
    "            print(\"Going through ResNet\")\n",
    "            audio_embeddings = audio_model(audio_stft)\n",
    "\n",
    "            # Concatenate the embeddings together.\n",
    "            concat_embeddings = (\n",
    "                torch.cat(\n",
    "                    (face_embeddings, frame_embeddings, audio_embeddings[:, None, :]),\n",
    "                    axis=1,\n",
    "                )\n",
    "                .float()\n",
    "                .to(device)\n",
    "            )\n",
    "\n",
    "            print(concat_embeddings.shape)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    random_seed = 41\n",
    "    metadata_path = (\n",
    "        \"/home/shu/cheapfake/cheapfake/contrib/wide_balanced_metadata_fs03.csv\"\n",
    "    )\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(\"Using device: {}\".format(device))\n",
    "\n",
    "    dfdataset = dataset.DeepFakeDataset(\n",
    "        metadata_path=metadata_path,\n",
    "        frame_transform=transforms.BatchRescale(4),\n",
    "        sequential_audio=True,\n",
    "        sequential_frames=True,\n",
    "        random_seed=random_seed,\n",
    "        num_samples=1,\n",
    "    )\n",
    "    dfdataloader = DataLoader(dfdataset, batch_size=1, shuffle=True)\n",
    "    checkpoint_path = \"./checkpoints\"\n",
    "\n",
    "    optimizer = 0\n",
    "    criterion = 0\n",
    "    num_epochs = 5\n",
    "\n",
    "    face_model = models.AugmentedFAN(device=device)\n",
    "    frame_model = models.AugmentedLipNet(device=device)\n",
    "    audio_model = resnet_models.ResNetSE34L()\n",
    "\n",
    "    train_model(\n",
    "        face_model=face_model,\n",
    "        frame_model=frame_model,\n",
    "        audio_model=audio_model,\n",
    "        dataloader=dfdataloader,\n",
    "        optimizer=optimizer,\n",
    "        criterion=criterion,\n",
    "        num_epochs=num_epochs,\n",
    "        checkpoint_path=checkpoint_path,\n",
    "    )\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
